Instruction for Workshop 2.5 Kubernetes in RealWorld:
Note: This instruction will start lab for kubernetes for real workshop:
URL: http://labs.play-with-k8s.com/
====================================================
Lab Description:
VMName:							Machine name		Roles:						IP Address:
node1       					node1       		Master						172.28.x.x
node2       					node2       		NodePort					172.28.x.x
node3					        node3		        NodePort					172.28.x.x
===================================================

0. Access K8S with url http://labs.play-with-k8s.com/: and Login with docker/github account

1. Click "+Add NEW Instance" for 3 Times
	
2. (node1) initial cluster by command:
    kubeadm init --kubernetes-version=v1.9.2 --pod-network-cidr=10.244.0.0/16 --token 8c2350.f55343444a6ffc46 --apiserver-advertise-address $(hostname -i)

	*Remark: output of this command will generate token that need to keep:
	-------------------------------------------------
	Sample Output:
	-------------------------------------------------
	Initializing machine ID from random generator.
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.9.2
[init] Using Authorization modes: [Node RBAC]
[preflight] Skipping pre-flight checks
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [node1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.23]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests"
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 30.501959 seconds
[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node node1 as master by adding a label and a taint
[markmaster] Master node1 tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 8c2350.f55343444a6ffc46
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 8c2350.f55343444a6ffc46 192.168.0.23:6443 --discovery-token-ca-cert-hash sha256:f46f209281c68c9fa49e98bd580acaf42381eef057d8a0ff7a474a4ffd32773e

Waiting for api server to startup..
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
daemonset "kube-proxy" configured
No resources found
	-------------------------------------------------
3. (node1) Run Command for Initial and Taint Node by command:
		  kubectl taint nodes --all node-role.kubernetes.io/master-

4. (node1) Create weave net plugin for network for cluster by command:
	kubectl apply -n kube-system -f \
    "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 |tr -d '\n')"

5. (node1) Check master readyness and dns by command (Take 5 - 10 min):
	kubectl get pods --all-namespaces
	kubectl describe pods <kube-dns name> --namespace kube-system

	-------------------------------------------------
	Sample Output
	-------------------------------------------------
	NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-kubeserve-ms                      1/1       Running   0          1m
kube-system   kube-apiserver-kubeserve-ms            1/1       Running   0          2m
kube-system   kube-controller-manager-kubeserve-ms   1/1       Running   0          3m
kube-system   kube-dns-2425271678-xhhk6              3/3       Running   0          2m
kube-system   kube-flannel-ds-n9ws7                  2/2       Running   0          2m
kube-system   kube-proxy-vzswv                       1/1       Running   0          2m
kube-system   kube-scheduler-kubeserve-ms            1/1       Running   0          2m
	-------------------------------------------------

6. (node1) Install dashboard by command:
curl -L -s https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml  | sed 's/targetPort: 8443/targetPort: 8443\n  type: LoadBalancer/' |   kubectl apply -f -
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/rbac/heapster-rbac.yaml
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/influxdb/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl create clusterrolebinding insecure-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
kubectl get pods --all-namespaces

7. (node1) Open dashboard by right click link and change to https:


8. (node1) Open browser by command
https://ip10-0-20-3-30711.host2.labs.play-with-k8s.com/ 
(Example)

9. (node2),(node3) ssh and join to cluster by command:
	kubeadm join --token 8c2350.f55343444a6ffc46 192.168.0.23:6443 --discovery-token-ca-cert-hash sha256:f46f209281c68c9fa49e98bd580acaf42381eef057d8a0ff7a474a4ffd32773e
    (Example)
	-------------------------------------------------
	Sample Output
	-------------------------------------------------
Initializing machine ID from random generator.
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Skipping pre-flight checks
[discovery] Trying to connect to API Server "192.168.0.23:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.0.23:6443"
[discovery] Requesting info from "https://192.168.0.23:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.0.23:6443"
[discovery] Successfully established connection with API Server "192.168.0.23:6443"
[bootstrap] Detected server version: v1.9.2
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.
	-------------------------------------------------

10. (node1) Check Node in Cluster by command (This take 5 - 10 min):
kubectl get nodes

-------------------------------------------------
Sample Output
-------------------------------------------------
NAME      STATUS    ROLES     AGE       VERSION
node1     Ready     master    12m       v1.8.4
node2     Ready     <none>    1m        v1.8.4
node3     Ready     <none>    1m        v1.8.4
-------------------------------------------------

11. (node1)Check Pods from all cluster system running by command:
kubectl get pods --all-namespaces

-------------------------------------------------
Sample Output
-------------------------------------------------
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-kubeserve-ms                      1/1       Running   0          10m
kube-system   kube-apiserver-kubeserve-ms            1/1       Running   0          10m
kube-system   kube-controller-manager-kubeserve-ms   1/1       Running   0          11m
kube-system   kube-dns-2425271678-xhhk6              3/3       Running   0          11m
kube-system   kube-flannel-ds-1h8cv                  2/2       Running   2          7m
kube-system   kube-flannel-ds-83xdr                  2/2       Running   0          6m
kube-system   kube-flannel-ds-n9ws7                  2/2       Running   0          11m
kube-system   kube-proxy-6d7g1                       1/1       Running   0          7m
kube-system   kube-proxy-qzfdr                       1/1       Running   0          6m
kube-system   kube-proxy-vzswv                       1/1       Running   0          11m
kube-system   kube-scheduler-kubeserve-ms            1/1       Running   0          11m
-------------------------------------------------

12. (node1) Test deployment basic nginx pods by command
kubectl run webtest --image=labdocker/nginx:latest --port=80
kubectl get pods -o wide
kubectl expose deployment webtest --target-port=80 --type=NodePort
kubectl get svc -o wide

13. (node1) Test open web for nginx

14. (node1) Cleanup Lab by command:
kubectl delete deployment/webtest
kubectl delete svc/webtest