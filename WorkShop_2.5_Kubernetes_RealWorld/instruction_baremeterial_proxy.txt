Instruction for Workshop 2.5 Kubernetes in RealWorld:
Note: This instruction will start lab for kubernetes for real workshop:
====================================================
Lab Description:
VMName:							Machine name		Roles:						IP Address:
kubernetes-ms					kubernetes-ms		Master						10.38.14.200
kubernetes-1					kubernetes-1		NodePort					10.38.14.201
kubernetes-2					kubernetes-2		NodePort					10.38.14.102

===================================================
Username: xxxx	
Password: xxxx

Step 0: Edit file initialsetup_proxy.sh for fill-in your proxy <username>,<password>,<proxy ip>,<proxy port>

===================================================
Part 1: Setup Docker and Kubernetest Binary: (Apply all node)
===================================================
1. SCP script (initialsetup_proxy.sh) from local to Kubernetes Machine (/home/<user>/initialsetup_proxy.sh):
	sudo dpkg --configure -a (if nessesary)

2. Start setup docker and kubernetes by command:
    sudo iptables-save > /tmp/iptables.conf
	sudo chmod +x *.sh
	./initialsetup_proxy.sh
	sudo iptables-restore < /tmp/iptables.conf
	
3. Create folder docker.service.d by command:
	sudo mkdir -p /etc/systemd/system/docker.service.d
	
4. Create file http-proxy.conf and configure proxy privilege by command: (docker_proxy.sh)

	sudo vi /etc/systemd/system/docker.service.d/http-proxy.conf
	
	---------------------------------------------------
	[Service]
	Environment="HTTP_PROXY=http://<proxy user>:<proxy password>@<proxy ip>:<proxy port>/"
	---------------------------------------------------

5. Flush change docker configruation and restart service
	sudo systemctl daemon-reload
	sudo systemctl restart docker
	
6. Relogon system and test docker command:  docker run hello-world

===================================================
Part 2: Initial Kubernetes Cluster
===================================================
1. (kubernetes-ms) initial cluster by command:
	export http_proxy="http://<proxy user>:<proxy password>@<proxy ip>:<proxy port>/"
	export https_proxy="http://<proxy user>:<proxy password>@<proxy ip>:<proxy port>/"
	export NO_PROXY=10.38.14.200
	sudo su -
	swapoff -a
	kubeadm init --kubernetes-version=v1.9.2 --pod-network-cidr=10.244.0.0/16 --token 8c2350.f55343444a6ffc46
	exit

	*Remark: output of this command will generate token that need to keep:
	-------------------------------------------------
	Sample Output:
	-------------------------------------------------
[init] Using Kubernetes version: v1.9.2
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.0-ce. Max validated version: 17.03
	[WARNING FileExisting-crictl]: crictl not found in system path
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [kubernetes-ms kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.38.14.200]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 88.502541 seconds
[uploadconfig]Â Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node kubernetes-ms as master by adding a label and a taint
[markmaster] Master kubernetes-ms tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 8c2350.f55343444a6ffc46
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 8c2350.f55343444a6ffc46 10.38.14.200:6443 --discovery-token-ca-cert-hash sha256:4d2839643129359f43c4fdfdda38718009494535ed5685940f6271b7d9791494
	-------------------------------------------------
2. (kubernetes-ms) Setup run cluster system by command (Regular User):
		  mkdir -p $HOME/.kube
		  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		  sudo chown $(id -u):$(id -g) $HOME/.kube/config

		  export NO_PROXY=10.38.14.200
		  kubectl taint nodes --all node-role.kubernetes.io/master-


4. (kubernetes-ms) Create flennel net plugin for network for cluster by command:
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.8.0/Documentation/kube-flannel-rbac.yml
	
5. Check master readyness and dns by command (Take 5 - 10 min):
	kubectl get pods --all-namespaces
	kubectl describe pods <kube-dns name> --namespace kube-system
	
	-------------------------------------------------
	Sample Output
	-------------------------------------------------
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-ubuntumaster                      1/1       Running   0          38m
kube-system   kube-apiserver-ubuntumaster            1/1       Running   0          38m
kube-system   kube-controller-manager-ubuntumaster   1/1       Running   0          38m
kube-system   kube-dns-2425271678-hh0br              3/3       Running   0          43m
kube-system   kube-flannel-ds-738kk                  1/1       Running   0          13m
kube-system   kube-proxy-tf8pm                       1/1       Running   0          43m
kube-system   kube-scheduler-ubuntumaster            1/1       Running   0          38m
	-------------------------------------------------
6. (kubernetes-ms) Install dashboard, heapster server by command:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/rbac/heapster-rbac.yaml
kubectl apply -f https://github.com/kubernetes/heapster/raw/master/deploy/kube-config/influxdb/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl create clusterrolebinding insecure-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard


7. (kubernetes-ms) Check readyness by command:
	kubectl get pods --all-namespaces

8. (Kubeserve-ms) Open dashboard by command:
kubectl proxy --address 10.38.14.200 --accept-hosts '.*'

9. (local) Open browser by command
http://10.38.14.200:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

10. (kubernetes-1),(kubernetes-2) ssh and join to cluster by command:
	export http_proxy="http://<proxy user>:<proxy password>@<proxy ip>:<proxy port>/"
	export https_proxy="http://<proxy user>:<proxy password>@<proxy ip>:<proxy port>/"
	export NO_PROXY=<ip of kubenetes1 / 2>
	sudo su -
	swapoff -a
	kubeadm join --token 8c2350.f55343444a6ffc46 10.38.14.200:6443 --discovery-token-ca-cert-hash sha256:4d2839643129359f43c4fdfdda38718009494535ed5685940f6271b7d9791494
	exit

	-------------------------------------------------
	Sample Output
	-------------------------------------------------
[preflight] Running pre-flight checks.
	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.0-ce. Max validated version: 17.03
	[WARNING FileExisting-crictl]: crictl not found in system path
[discovery] Trying to connect to API Server "10.38.14.200:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://10.38.14.200:6443"
[discovery] Requesting info from "https://10.38.14.200:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "10.38.14.200:6443"
[discovery] Successfully established connection with API Server "10.38.14.200:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
	-------------------------------------------------

11. (kubernetes-ms) Check Node in Cluster by command (This take 5 - 10 min):
kubectl get nodes

-------------------------------------------------
Sample Output
-------------------------------------------------
NAME      		STATUS      AGE       VERSION
kubernetes-1     Ready      12m       v1.9.2
kubernetes-2     Ready      1m        v1.9.2
kubernetes-ms    Ready      1m        v1.9.2
-------------------------------------------------

12. (kubernetes-ms)Check Pods from all cluster system running by command:
kubectl get pods --all-namespaces

-------------------------------------------------
Sample Output
-------------------------------------------------
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-ubuntumaster                      1/1       Running   0          42m
kube-system   kube-apiserver-ubuntumaster            1/1       Running   0          42m
kube-system   kube-controller-manager-ubuntumaster   1/1       Running   0          42m
kube-system   kube-dns-2425271678-hh0br              3/3       Running   0          47m
kube-system   kube-flannel-ds-738kk                  1/1       Running   0          17m
kube-system   kube-flannel-ds-8vqzx                  1/1       Running   1          2m
kube-system   kube-flannel-ds-kk51k                  1/1       Running   0          2m
kube-system   kube-proxy-cw698                       1/1       Running   0          2m
kube-system   kube-proxy-jv01t                       1/1       Running   0          2m
kube-system   kube-proxy-tf8pm                       1/1       Running   0          47m
kube-system   kube-scheduler-ubuntumaster            1/1       Running   0          42m
-------------------------------------------------

13. (kubernetes-ms) Test deployment basic nginx pods by command
kubectl run webtest --image=labdocker/nginx:latest --port=80
kubectl get pods -o wide
kubectl expose deployment webtest --target-port=80 --type=NodePort
kubectl get svc -o wide

14. (kubernetes-ms) Test get web inside farm by curl:
curl http://10.38.14.200:<port>
curl http://10.38.14.201:<port>
curl http://10.38.14.202:<port>


15. (kubeserve-ms) Cleanup Lab by command:
kubectl delete deployment/webtest
kubectl delete svc/webtest



